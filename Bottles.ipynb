{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasrasa/Industrial-projects/blob/main/Bottles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIzICjaph_Wy"
      },
      "source": [
        "<a align=\"center\" href=\"https://hub.ultralytics.com\" target=\"_blank\">\n",
        "<img width=\"1024\", src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\"></a>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <a href=\"https://github.com/ultralytics/hub/actions/workflows/ci.yaml\">\n",
        "    <img src=\"https://github.com/ultralytics/hub/actions/workflows/ci.yaml/badge.svg\" alt=\"CI CPU\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/hub/blob/master/hub.ipynb\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "Welcome to the [Ultralytics](https://ultralytics.com/) HUB notebook!\n",
        "\n",
        "This notebook allows you to train [YOLOv5](https://github.com/ultralytics/yolov5) and [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€ models using [HUB](https://hub.ultralytics.com/). Please browse the YOLOv8 <a href=\"https://docs.ultralytics.com\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/hub/issues/new/choose\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRQ2ow94MiOv"
      },
      "source": [
        "## introduction\n",
        "\n",
        "In this project I transfered learning a medium YOLOv8 model (YOLOv8m) and trained it on a small custome dataset of about 40 images.\n",
        "\n",
        "I annotate the dataset and use ultralytics to train the YOLOv8 model. The model is saved in two formats, PyTorch(.pt) and ONNX(.onnx) and predictions are made using both.\n",
        "\n",
        "The model is then tested on a unlabelled set and I visually checked the results.\n",
        "\n",
        "## The ultimate goal of this project is to deploy this model on an actual automation line for robot pick and place.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyDnXd-n4c7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af133197-7573-490f-a588-26167e98cfcf"
      },
      "source": [
        "#let's start by installing the ultralytics library and import the necessary classes\n",
        "\n",
        "%pip install ultralytics  # install\n",
        "from ultralytics import YOLO, checks, hub\n",
        "checks()  # checks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.5/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ9BwaAqxAm4"
      },
      "source": [
        "# Start\n",
        "\n",
        "My dataset is on my google drive so I have to mount the drive to colab to be able to train the model on my dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount my google drive in colab\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tpZZUWI_Nv9",
        "outputId": "9cc6d52f-5cf4-40f4-8f31-5127fd4f9562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I already have trained my model using the ultralytics web interface and already deployed my model in two .pt and .onnx formats.\n",
        "\n",
        "**One thing I noticed here is that importing and using the pytorch model and making prediction with it is more intuitive and easier than the onnx model.**"
      ],
      "metadata": {
        "id": "tR2K7vprbxgB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSlZaJ9Iw_iZ"
      },
      "source": [
        "# let's load in the saved model from google drive\n",
        "\n",
        "model = YOLO(\"/content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/Bottles.pt\") #load in the PyTorch model in\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is time to test the trained model on an unlabelled set of images to see the results of how accurate the model is.\n",
        "\n",
        "*Note that this object detector is only required to detect only one class of objects."
      ],
      "metadata": {
        "id": "oSg0mklWdkBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import exception\n",
        "from os.path import isdir\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "Root_Dir = \"/content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles\" #my root directory in my drive\n",
        "unlabelled = os.path.join(Root_Dir,\"unlabelled\") #the unlabelled directory\n",
        "unlabelled_labelled_pt = os.path.join(Root_Dir,\"unlabelled_labelled_pt\") #the labelled results"
      ],
      "metadata": {
        "id": "HPejp6oK7PCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is a little script that loads the unlabelled images\n",
        "#predicts the objects and saves them in another directory\n",
        "try:\n",
        "  if not os.path.isdir(unlabelled_labelled_pt):\n",
        "    os.mkdir(unlabelled_labelled_pt) #make the directory if it is not there\n",
        "\n",
        "  for img_path in os.listdir(unlabelled):\n",
        "    results = model.predict(os.path.join(unlabelled,img_path),imgsz = 640, conf = 0.8) #predict the image with the model\n",
        "\n",
        "    for i,r in enumerate(results):\n",
        "        im_array = r.plot()  # plot a BGR numpy array of predictions\n",
        "        im = Image.fromarray(im_array,'RGB')  # RGB PIL image\n",
        "        im.save(os.path.join(unlabelled_labelled_pt,img_path)) #save the results with overlay\n",
        "\n",
        "except ValueError:\n",
        "  print(f\"There is no such File or Directory such as {Root_Dir}\")"
      ],
      "metadata": {
        "id": "1q8ilqpCel0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf37477-0f6b-4413-c199-26238413b06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-05-604.png: 480x640 19 bottles, 114.1ms\n",
            "Speed: 11.8ms preprocess, 114.1ms inference, 32.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-09-828.png: 480x640 18 bottles, 28.2ms\n",
            "Speed: 2.3ms preprocess, 28.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-14-057.png: 480x640 17 bottles, 28.3ms\n",
            "Speed: 2.3ms preprocess, 28.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-17-883.png: 480x640 16 bottles, 28.2ms\n",
            "Speed: 2.6ms preprocess, 28.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-22-518.png: 480x640 15 bottles, 28.3ms\n",
            "Speed: 3.8ms preprocess, 28.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-30-843.png: 480x640 14 bottles, 28.2ms\n",
            "Speed: 2.5ms preprocess, 28.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-34-771.png: 480x640 13 bottles, 28.4ms\n",
            "Speed: 3.3ms preprocess, 28.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-43-759.png: 480x640 12 bottles, 28.4ms\n",
            "Speed: 3.4ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-50-089.png: 480x640 11 bottles, 28.2ms\n",
            "Speed: 3.3ms preprocess, 28.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-33-53-897.png: 480x640 10 bottles, 28.2ms\n",
            "Speed: 3.4ms preprocess, 28.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-34-01-266.png: 480x640 9 bottles, 28.3ms\n",
            "Speed: 4.2ms preprocess, 28.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-34-05-228.png: 480x640 8 bottles, 28.3ms\n",
            "Speed: 2.5ms preprocess, 28.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-34-10-519.png: 480x640 7 bottles, 28.3ms\n",
            "Speed: 2.3ms preprocess, 28.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-34-14-376.png: 480x640 6 bottles, 28.2ms\n",
            "Speed: 2.2ms preprocess, 28.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-34-20-331.png: 480x640 5 bottles, 28.2ms\n",
            "Speed: 2.3ms preprocess, 28.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-34-24-245.png: 480x640 4 bottles, 28.2ms\n",
            "Speed: 2.4ms preprocess, 28.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/unlabelled/S70_LoadBufferBottles_ Pass.2023-04-20T09-35-07-266.png: 480x640 3 bottles, 28.2ms\n",
            "Speed: 2.3ms preprocess, 28.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "just show one example of how the model performed on some unseen cases"
      ],
      "metadata": {
        "id": "iazyvmbfrntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now I just show one of the unseen pictures that the model predicted\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "for dir,_,files in os.walk(unlabelled_labelled_pt):\n",
        "  img = cv2.imread(os.path.join(dir,files[0]))"
      ],
      "metadata": {
        "id": "P0B0jiMFrmzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous section of the code uses the pytorch model and the predictions and using the models are easier and faster in colab.\n",
        "However, the power of onnx models are that you can convert them to any models and increases the compatibility of the model with different systems. Additionally, due to the approximations made during conversion in onnx models the computational time assumes to be lower which would benefit my case for deploying this model on an automation line even more.\n",
        "\n",
        "Therefore, I try to import and predict the same set of images with the onnx model as well."
      ],
      "metadata": {
        "id": "rrTLbyx-umId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first let's install onnxruntime\n",
        "# Since I am processing an image I would assume that gpu processing will speed up my process\n",
        "# I will have to try the cpu and check the runtime to make sure\n",
        "!pip install onnxruntime-gpu\n",
        "!pip install torch # since some of the onnx libraries(onnx) are in the torch library\n",
        "!pip install tf2onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNz8B58QTKzO",
        "outputId": "1cd02911-8829-4109-b91c-c0a03d0a7bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.4/153.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.16.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.15.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m454.7/454.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.23.5)\n",
            "Collecting onnx>=1.4.1 (from tf2onnx)\n",
            "  Downloading onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (23.5.26)\n",
            "Requirement already satisfied: protobuf~=3.20.2 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.4.1->tf2onnx) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2023.7.22)\n",
            "Installing collected packages: onnx, tf2onnx\n",
            "Successfully installed onnx-1.14.1 tf2onnx-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "onnx_model = onnx.load(os.path.join(Root_Dir,\"Bottles.onnx\")) # load in the onnx model\n",
        "print(onnx.checker.check_model(onnx_model))# to Check the consistency of a model. None means there is no exceptions and everything is good"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiPaNa67wdT-",
        "outputId": "88adbedb-4b6a-4b23-e9e6-848a61761d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a onnxruntime session\n",
        "ort_sess = ort.InferenceSession(\"/content/gdrive/MyDrive/Computer Vision/YOLOV8/Bottles/Bottles.onnx\",providers=ort.get_available_providers())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDyPPsVVbm2F",
        "outputId": "b5f63bb1-f915-433c-dbb5-60f0635aaf4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP Error /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1193 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A great way to get know what the input and output of a onnx model looks like is the two cells below.\n",
        "\n",
        "I use the next two cells, which you can find on the onnxruntime [webpage](\"https://onnxruntime.ai/docs/api/python/auto_examples/plot_load_and_predict.html\"), to manipulate the unlabelled images and process them with my model."
      ],
      "metadata": {
        "id": "saXeLSzdb7NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get the name, shape and type of our inputs\n",
        "input_name = ort_sess.get_inputs()[0].name\n",
        "print(\"input name\", input_name)\n",
        "input_shape = ort_sess.get_inputs()[0].shape\n",
        "print(\"input shape\", input_shape)\n",
        "input_type = ort_sess.get_inputs()[0].type\n",
        "print(\"input type\", input_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t04kS8lF890P",
        "outputId": "94909356-0135-4301-eb50-b33fc7ce1844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input name images\n",
            "input shape [1, 3, 640, 640]\n",
            "input type tensor(float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see what to expect as our outputs\n",
        "output_name = ort_sess.get_outputs()[0].name\n",
        "print(\"output name\", output_name)\n",
        "output_shape = ort_sess.get_outputs()[0].shape\n",
        "print(\"output shape\", output_shape)\n",
        "output_type = ort_sess.get_outputs()[0].type\n",
        "print(\"output type\", output_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZOK9LEW9ANv",
        "outputId": "da6c4617-57ff-426c-9fd4-12bf36181ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output name output0\n",
            "output shape [1, 5, 8400]\n",
            "output type tensor(float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I know what the input to my onnx model should be like I will import and manipulate the image to feed into my onnx model.\n",
        "\n",
        "The output of the onnx model is a list of boxes that it detected. The data needs to be processed and similar or under confidence threshold coordinates need to be removed. Finally the end result would be the desired boxes.\n",
        "\n",
        "Using the processed coordinates the image with boxes as overlay can be shown."
      ],
      "metadata": {
        "id": "DFXznqHSd9aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's read in the output and massage it into the shape that our model can take\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for dir,_,files in os.walk(unlabelled_labelled_pt):\n",
        "  img = cv2.imread(os.path.join(dir,files[0])) #load in the first image from the unlabelled directory\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img, (640,640), interpolation=cv2.INTER_AREA) #resize the image to the input shape of the model\n",
        "  img = np.expand_dims(img,axis=0).astype(np.float32)/255. #expand the dimensions of the image\n",
        "  img = np.transpose(img,[0,3,1,2]) #reshape the image into the shape that the model can take\n",
        "  outputs = ort_sess.run(None,{input_name: img}) #run the model and predict the outputs\n",
        "\n",
        "Boxes = outputs[0]\n",
        "print(Boxes.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACbS19c1VZLK",
        "outputId": "e33f3e1b-b7ca-4db6-8222-d29da2c360ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 8400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's process the data\n",
        "print(Boxes[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyUJBtIiLHtd",
        "outputId": "0b118e5f-8551-4038-ddee-8da82c8778dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[     7.3623      9.8955      20.478 ...      549.42      579.49       600.8]\n",
            " [      14.81      14.291      12.146 ...      607.14       607.3      604.84]\n",
            " [     18.387      23.116       39.48 ...      249.12       259.9      218.47]\n",
            " [      30.18      29.715      26.257 ...      146.51      144.83      133.52]\n",
            " [ 0.00064427  0.00021416  2.1458e-05 ...  9.5963e-05  8.8811e-05  6.4969e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWxXLYpLLQra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}